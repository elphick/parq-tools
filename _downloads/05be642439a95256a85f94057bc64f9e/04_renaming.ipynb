{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Renaming and Metadata\n\nA simple example to demonstrate how to rename columns in a parquet file.\nAdditionally, we can update the metadata in the file - in this case we add column descriptions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import json\nimport tempfile\n\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nfrom pathlib import Path\n\nfrom parq_tools import rename_and_update_metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a Parquet file\n\nCreate a temporary parquet file for demonstration\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def create_parquet_file(file_path: Path):\n    # Define the dataset\n    data = {\n        \"x\": range(1, 11),  # Index column\n        \"y\": range(11, 21),  # Index column\n        \"z\": range(21, 31),  # Index column\n        \"a\": [f\"val{i}\" for i in range(1, 11)],  # Supplementary column\n        \"b\": [i * 2 for i in range(1, 11)],  # Supplementary column\n        \"c\": [i % 3 for i in range(1, 11)],  # Supplementary column\n    }\n\n    # Create a DataFrame\n    df = pa.Table.from_pydict(data)\n\n    # Write the DataFrame to a Parquet file\n    pq.write_table(df, file_path)\n\n\nparquet_file_path = Path(tempfile.gettempdir()) / \"example_data.parquet\"\ncreate_parquet_file(parquet_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "View the file as a DataFrame\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = pd.read_parquet(parquet_file_path)\ndf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rename columns\n\nWe can rename a selection of columns.  Here we assume we don't want to rename the `index` columns.\nAssuming we have no knowledge of the column names, we'll read them from the file schema.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "index_cols = [\"x\", \"y\", \"z\"]\ncol_names = pq.ParquetFile(parquet_file_path).schema.names\ncol_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a mapping and rename the columns\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "new_col_names: dict[str, str] = {col: f\"new_{col}\" for col in col_names if col not in index_cols + ['c']}\noutput_file_path = parquet_file_path.parent / \"renamed_data.parquet\"\nrename_and_update_metadata(input_path=parquet_file_path, output_path=output_file_path,\n                           rename_map=new_col_names, show_progress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read the renamed file and display it\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_renamed = pd.read_parquet(output_file_path)\ndf_renamed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Update metadata\nWe can also update the metadata in the file.  In this case we add descriptions to the renamed columns.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "metadata = {\n    \"description\": \"This is the description of the dataset\",\n    \"version\": \"0.1.0\",\n}\ncolumn_descriptions = {'new_a': {'description': \"This is the a column renamed\"},\n                       'new_b': {'description': \"This is the b column renamed\"},\n                       'c': {'description': \"This is the original c column\",\n                             'unit_of_measure': \"unitless\"}}\nrename_and_update_metadata(input_path=output_file_path, output_path=output_file_path,\n                           rename_map=new_col_names, table_metadata=metadata, column_metadata=column_descriptions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First the file metadata\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "d_metadata = pq.read_metadata(output_file_path)\nd_metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now the table metadata\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pf: pq.ParquetFile = pq.ParquetFile(output_file_path)\ntable_metadata = pf.schema.to_arrow_schema().metadata\ndecoded = {k.decode(): v.decode() for k, v in table_metadata.items()}\nprint(json.dumps(decoded, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now the column metadata\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "arrow_schema = pf.schema.to_arrow_schema()\n# Extract column metadata\ncolumn_metadata = {col: arrow_schema.field(col).metadata for col in pf.schema.names}\n# Decode metadata\ncolumn_metadata_decoded = {\n    col: {k.decode(): v.decode() for k, v in meta.items()} if meta else {}\n    for col, meta in column_metadata.items()\n}\nprint(json.dumps(column_metadata_decoded, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Persisting only renamed columns\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "new_col_names = {\"x\": \"x\", \"a\": \"new_a\"}\noutput_file_path_renamed_only = parquet_file_path.parent / \"renamed_data_only.parquet\"\nrename_and_update_metadata(input_path=parquet_file_path, output_path=output_file_path_renamed_only,\n                           rename_map=new_col_names, return_all_columns=False)\n\ndf_renamed_only = pd.read_parquet(output_file_path_renamed_only)\ndf_renamed_only"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}